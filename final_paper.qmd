---
title: "Predicting when two neurons will form a synapse"
author:
  - id: pgordy
    name: Prayag Gordy
    affiliations:
      - name: Rice University
        department: ELEC 478
        city: Houston
        country: United States
format:
  ieee-pdf: default
bibliography: references.bib
nocite: |
  @*
abstract: |
  I used a stacking model with logistic regression, random forest, and gradient-boosted trees to predict when two neurons will synapse or form a connection. First, I used linear regression models to impute missing morphological embedding data. Then, I constructed a suite of new features, including additional distance estimates, categories of projection regions, percentiles of neuron popularity, and similarity and norm metrics for morphological embeddings and feature weights. I tuned five initial models: (1) logistic regression using a grid search over two hyperparameters; (2) random forest using a grid search over three hyperparameters; (3) random forest using Bayesian optimization built on (2); (4) boosted trees using a grid search with Latin hypercube sampling over seven hyperparameters; and (5) boosted trees using Bayesian optimization built on (4). To determine the final classification, I took the mode of the predictions from each of the five models. Though my stacking model would have ranked 47th on the public leaderboard, it ranked fifth on the private leaderboard, suggesting that my approach reduced overfitting.
execute: 
  eval: true
  echo: false
---

# Introduction

[T]{.IEEEPARstart}[he]{} overall goal of this competition was to predict when two neurons will synapse or form a connection. As described in the competition guidelines, each observation in the data is a pair of neurons in axonaldendritic proximity (ADP), a necessary but not sufficient criterion to form a synapse. Each synapse occurs between a pre-synaptic neuron — whose axon is involved in the synapse — and a post-synaptic neuron (dendrite). Neurons can form synapses with multiple other neurons, and can form multiple synapses with a single other neuron. Neurons can be pre-synaptic in one ADP and post-synaptic in another. It would not make sense for a neuron to synapse with itself.

The distance between the axonal and dendritic ADP coordinates was provided, but other potential distances of interest were not. Given that our entire estimation procedure is a conditional one — when do neurons form a synapse _given_ they are in close proximity — I engineered features representing additional potential distances of interest.

Computational resources were likely to be strained, so I avoided introducing too many additional features. I also sought to compress the 32 morphological embedding and 512 feature weight measurements — which are joined with both of the pre-synaptic and post-synaptic neurons — to reduce the number of predictors and to avoid overfitting. I used three-fold cross-validation with creative tuning approaches such as Bayesian optimization and Latin hypercube sampling to explore as much of the hyperparameter space as realistically possible with constrained computation time. Then, I took the mode of the predictions of each of five models to finalize my submission.

# Methods

```{r}
#| message: false
library(ggplot2)

raw_train <- readr::read_csv("neuron-synapse-prediction/train_data.csv") |>
	dplyr::mutate(connected = forcats::as_factor(connected) |>
									forcats::fct_relevel(c("TRUE", "FALSE")))
feature_weights <- readr::read_csv("neuron-synapse-prediction/feature_weights.csv")
morph_embeddings <- readr::read_csv("neuron-synapse-prediction/morph_embeddings.csv")
```

```{r}
missing_pre_morph <- raw_train |>
	dplyr::select(ID, pre_nucleus_id, post_nucleus_id) |>
	dplyr::left_join(morph_embeddings |>
									 	dplyr::select(nucleus_id, morph_pre = morph_emb_0),
									 by = c("pre_nucleus_id" = "nucleus_id")) |>
	dplyr::summarize(morph_pre = sum(is.na(morph_pre))/dplyr::n()) |>
	dplyr::pull()
```

## Feature engineering

### Imputation

My first feature engineering step was to impute the missing morphological embedding data. Though none of the post-synaptic neuron morphological embedding data were missing, we did not have morphological embeddings for `r scales::percent(missing_pre_morph)` of the pre-synaptic neurons. Instead of dropping rows, I imputed since some of the methods I used require no missing values.

There are many imputation methods, including but not limited to tree models, $K$ nearest-neighbors, and mean assignment. I chose linear regression imputation. Crucially, I did not have any neuroscientific understanding of these morphological embeddings, so I was operating somewhat in the dark. I noticed that each morphological embedding on its own looks to have a bell-shaped distribution with a mean around $0$ and a range from $-2$ to $2$, and that I could not discern many other patterns. I could have chosen a variety of imputation methods, each with their own hyperparameters, and tuned and validated them, but the marginal benefit to this intensive approach seemed minor. Linear regression is fast and simple, and especially because I was using summaries of each morphological embedding, I was satisfied. I fit the linear regression models only on the training dataset, using all predictors from the given training dataset and the features I engineered, except for anything related to the morphological embeddings and the feature weights.

### Distance metrics

Distance is clearly important in predicting when two neurons will form a synapse. Since we have multiple coordinates for each neuron, I decided to compute some of the relevant distances. The distance between the axonal and dendritic ADP coordinates was already given as `adp_dist`.

I computed the Euclidean distances between the axonal ADP and the post-synaptic nucleus, the dendritic ADP and the pre-synaptic nucleus, the pre-synaptic nucleus and post-synaptic nucleus, and the pre-synaptic readout location and post-synaptic readout location.

### Similarities and norms

I wanted to use as much information from the feature weights and morphological embeddings as possible without including every single predictor, which would amount to `r scales::comma((ncol(feature_weights) + ncol(morph_embeddings) - 2) * 2)` additional features. I settled on eight columns, four for feature weights and four for morphological embeddings. I computed the cosine similarity between the measures on the pre-synaptic and post-synaptic neurons. I also computed the $L_2$-norm of each of the pre-synaptic measures, the post-synaptic measures, and the element-wise difference between the pre-synaptic and post-synaptic measures.

Again, I brought no neuroscientific knowledge to this approach. Based on the competition documentation, I wondered if the magnitude of the feature weights and morphological embeddings mattered (since it said the sign was arbitrary). I also suspected that pre-synaptic and post-synaptic neurons with similar measures may be more likely to be connected, hence the norm of the element-wise difference.

### Popularity metrics

I wondered if the amount of ADPs a neuron is involved in could predict if it will form a synapse. I decided to include two "popularity" metrics. First, I looked at how many times a given pair of a pre-synaptic neuron and a post-synaptic neuron are in an ADP as a sort of "within" metric. Then, I considered how distinct post-synaptic neurons a pre-synaptic neuron is in an ADP with, and vice-versa, as a "between" metric. Since the size of the dataset may vary, I converted these counts to percentiles.

I thought a lot about the reliability of these features. If the test dataset is very small, for instance, computing these popularity metrics on only the testing data may yield weird results. However, the size of the training and leaderboard data in this project alleviated my concerns in this instance. Other applications may merit different approaches.

### Other feature engineering

As suggested in the example, I constructed a projection region variable that is a string concatenation of the pre-synaptic and post-synaptic brain areas. I also introduced a Boolean flag that is true if the pre- and post-regions are different.

Crucially, I recognized that we had a large class imbalance in the training data: Only `r scales::percent(sum(raw_train$connected == "TRUE")/nrow(raw_train), accuracy = 0.1)` of the training data observations were connected. There are many ways to solve this, and I chose to oversample the connected observations with a simple bootstrap. I also considered undersampling the majority class, but this would drastically reduce the size of our training data. I could have used SMOTE, which generates synthetic samples of the minority class. However, I was wary of this and other synthetic options because I have so little domain knowledge, and I wanted to minimize this sort of guess-work. Though this simple bootstrap may overfit to the training data, I decided it was my best option.

## Penalized logistic regression

Penalized logistic regression was the most simple model I fit. Logistic regression is a great first classifier, and I wanted to establish some sort of baseline performance. I introduced hyperparameter tuning by varying the penalty parameter $\lambda$ and by varying $\alpha$, the balance between a lasso and ridge setup, resulting in an elastic net. I tuned over a grid of 30 points with three-fold cross-validation, saving the best model in terms of the balanced accuracy metric, _i.e._, the average of the sensitivity and specificity.

## Random forest {#sec-rf}

Then, I stepped up to a random forest. Some existing research suggests that random forests are the best classifiers [@RFsoGood], and from our exercises in class and on problem sets it is clear that they perform well. There is also a lot of room to tune these models, which I found useful in this setup.

I first tuned my random forest model over three hyperparameters: the number of predictors randomly sampled for each decision tree, the number of decision trees, and the minimum node size required for another split. From my initial testing, I noticed that a large number of short trees seemed to perform well, so I created a six-point grid ranging from 2,200 to 4,000 trees and 12,000 to 22,000 data points as the minimum node size. I tried between $3$ and $6$ predictors for each tree.

After this short tuning was completed, I built on those results using Bayesian hyperparameter optimization. This method attempts to balance exploration and exploitation in the hyperparameter space [@TMWR]. That is, it is important to _explore_ across the space to get out of local minima; at the same time, hyperparameter combinations nearby well-performing ones will tend to result in strong models, hence, _exploitation_. Bayesian hyperparameter optimization balances exploration and exploitation through an acquisition function, which is generally expected improvement [@BOP]. Because Bayesian hyperparameter optimization works around a Gaussian process model, we can take advantage of probability distributions to determine a point in the hyperparameter space that is expected to most improve upon the current best output [@WUSTL].

I chose to run the Bayesian tuning for 10 iterations, implemented with the `tune` package from the `tidymodels` framework [@tune]. I once again set bounds on the hyperparameters to somewhat limit exploration. Specifically, I limited the number of predictors for each decision tree to be between $2$ and $10$, the number of trees between 2,000 and 6,000, and the minimum node size to split between 15,000 and 40,000.

At the end of the process, I had considered $16$ hyperparameter combinations. I saved the best model from the grid search and the best model from the Bayesian hyperparameter optimization.

## Gradient-boosted decision trees

Next, I used gradient boosting to fit another type of tree model in which we iteratively improve the ensemble by introducing new trees trained with the residuals of previous trees. I will use a `tidymodels` wrapper around the `xgboost` package implementation, which provides many hyperparameters to tune, of which I will focus on seven [@xgboost]:

1. the number of trees in the ensemble;
1. the depth of each of those trees;
1. the minimum node size required for another split;
1. the minimum reduction in the loss function required for another split;
1. the proportion of observations sampled at each iteration;
1. the number of predictors randomly sampled for each decision tree; and
1. the learning rate.

In addition to determining the shape of the trees in the ensemble, many of these hyperparameters focus on avoiding overfitting by making the boosting process more conservative and each tree weaker.

Once again, I started with tuning these seven hyperparameters over a grid. Instead of a traditional grid, however, I used the `dials` package implementation of Latin hypercube sampling to find an efficient grid that would be able to fill the hyperparameter space more efficiently [@dials]. Latin hypercube sampling partitions the hyperparameter space into, in my case, 30 different strata with equal probability, then selects a representative value from each [@LHS]. A goal of Latin hypercube sampling is often to maximize the distance between each point in an effort to fill the space [@saves2023smt].

We can see the space-filling property by graphing the initial hyperparameter grid. Because of space constraints, I will only plot three of the hyperparameters, with learning rate in its $\log_{10}$ units.

```{r}
load("grid_results/tune_bayes_bt_04_46_40.Rds")
```

```{r}
#| label: fig-bt-lhs
#| fig-height: 6
#| fig-cap: Latin hypercubing sampling results for three gradient-boosted decision tree model hyperparameters
grids <- res_bayes_bt |>
	workflowsets::collect_metrics() |>
	dplyr::mutate(learn_rate = log10(learn_rate),
								sampling_stage = ifelse(.iter == 0, "Latin hypercube", "Bayesian optimization")) |>
	dplyr::rename(`# trees` = trees, `# predictors` = mtry, `Learn rate` = learn_rate)

ggplot(grids, aes(x = .panel_x, y = .panel_y, color = sampling_stage)) + 
  geom_point() +
  geom_blank() +
	# geom_path(data = grids |> dplyr::filter(sampling_stage == "Bayesian optimization"),
	# 					mapping = aes(x = .panel_x, y = .panel_y),
	# 					inherit.aes = F,
	# 					show.legend = F) +
  ggforce::facet_matrix(vars(`# trees`, `# predictors`, `Learn rate`), layer.diag = 2) +
	labs(color = "Sampling stage") +
	scale_color_manual(values = c("Latin hypercube" = "chartreuse4", "Bayesian optimization" = "darkgoldenrod2")) +
	theme_minimal() +
	theme(legend.position = "top")
```

As the plot shows, Latin hypercube sampling (in green) does not result in a neat grid. Instead, the space is more filled, including with some points reaching the edges of the hyperparameter space. We also see some gaps, which is partly a product of ploting a seven-dimensional grid in two dimensions — the method focuses on filling the entire space, not just any two hyperparameters' space. Using a traditional grid with only 30 points would not let us test so many different setups nearly as well as this Latin hypercube sampling scheme.

Though I could not find literature on the topic, my intuition tells me that the Latin hypercube sampling method is a great first stage to pass to Bayesian optimization. As discussed in @sec-rf, Bayesian hyperparameter tuning focuses on exploration and exploitation. The Latin hypercube scheme does much of the overall exploration, letting the Bayesian optimization focus on exploitation in the open pockets of the hypercube. The orange points show the 10 hyperparameter tuples tested in the Bayesian tuning process; a few explore away from the Latin hypercube samples, but fill gaps between previous draws.

Like with the random forests, I saved two final models here: the best model from the Latin hypercube sampling scheme, and the best model after Bayesian optimization.

I also retained the variable importance from the final model fit after Bayesian optimization. `xgboost` provides two measures of variable importance: gain and cover. Gain is the improvement in accuracy that a feature provides — that is, how much better the classification is after a tree splits on that feature. Cover focuses on the _gradient_ part of the gradient boosting, representing the importance of a feature in minimizing the loss function. These two measures generally track very closely.

## Stacking


# Results

## Prediction

My first submission to the leaderboard was the predictions from a very simple unpenalized logistic regression model, which would have placed me 44th on the final public leaderboard. Then, I submitted my random forest model tuned on a grid, which would have placed 32nd. Tuning a gradient-boosting model with a grid moved me to 10th, and using Bayesian hyperparameter tuning placed me third. My stacked model would have placed me 24th.

I submitted the Bayesian-tuned boosting model and the stacked model for the private leaderboard. The former would have dropped me 15 places to 18th, while the latter jumped 19 places to fifth.

## Interpretability

Interpreting variable importance in a stacked model is difficult, even more so because of my imperfect implementation. Thus, I will look at variable importance in the boosted model, which was the next-best performer.

```{r}
load("final_models/tune_bayes_bt_final_15_39_55.Rds")
```

```{r}
#| label: fig-xg-vip
#| fig-height: 6
#| fig-cap: Variable importance of gradient-boosted trees
importance_xgb <- xgboost::xgb.importance(model = (final_bt_res |> 
																									 	workflowsets::extract_fit_parsnip() |> 
																									 	purrr::pluck("fit"))) |>
	tibble::as_tibble() |>
	janitor::clean_names() |>
	dplyr::mutate(default = ifelse(stringr::str_detect(feature, stringr::str_c(colnames(raw_train), collapse = "|")),
																 "Existing",
																 "Engineered"),
								feature = forcats::as_factor(feature) |>
									forcats::fct_reorder(gain))

importance_xgb |>
	dplyr::slice(1:20) |>
	ggplot(aes(x = gain, y = feature, fill = default)) +
	geom_bar(stat = "identity") +
	labs(x = "Variable importance",
			 y = NULL,
			 fill = "Predictor type") +
	theme_minimal() +
	theme(legend.position = "top") +
	scale_fill_manual(values = c("Engineered" = "indianred", "Existing" = "cadetblue3"))
```

@fig-xg-vip shows the variable importance for the `r nrow(importance_xgb)` strongest predictors. The blue bars represent features that were in the original dataset, and the red bars represent features that I engineered — `r sum(importance_xgb$default == "Engineered")` of these `r nrow(importance_xgb)`.

As I suspected, distance metrics were important across the board. The most important predictor overall was the distance between the axonal and dendritic ADPs, and my Euclidean estimates of other distances came close behind. The "within" popularity metric was also relatively important. I was surprised to see that only three of my eight summaries of the morphological embeddings and feature weights came in the top `r nrow(importance_xgb)`.

I used the gain measure from `xgboost`. The correlation between the gain and cover importance metrics was `r cor(importance_xgb$gain, importance_xgb$cover)`, so my results would have been very similar either way.

# Discussion

Discuss.

It is also worth noting that `finetune` provides many other tuning methods [@finetune]. For instance, I could have chosen simulated annealing or a racing method.


# Acknowledgements {-}

I worked alone and completed every step myself. I wrote all code in R and primarily fit models using the `tidymodels` framework [@tidymodels]. I wrote this final paper in Quarto using an IEEE theme from David Folio [-@TemplateIEEE].


# References {-}

