---
title: "Predicting when two neurons will form a synapse"
author:
  - id: pgordy
    name: Prayag Gordy
    affiliations:
      - name: Rice University
        department: ELEC 478
        city: Houston
        country: United States
format:
  ieee-pdf: default
bibliography: references.bib
nocite: |
  @*
abstract: |
  I used a stacking model with logistic regression, random forest, and gradient-boosted trees to predict when two neurons will synapse or form a connection. First, I used linear regression models to impute missing morphological embedding data. Then, I constructed a suite of new features, including additional distance estimates, categories of projection regions, percentiles of neuron popularity, and similarity and norm metrics for morphological embeddings and feature weights. I tuned five initial models: (1) logistic regression using a grid search; (2) random forest using a grid search; (3) random forest using Bayesian optimization built on (2); (4) boosted trees using a grid search with Latin hypercube sampling; and (5) boosted trees using Bayesian optimization built on (4). To determine the final classification, I took the mode of the predictions from each of the five models. Though my stacking model would have ranked 47th on the public leaderboard, it ranked fifth on the private leaderboard, suggesting that my approach reduced overfitting.
execute: 
  eval: true
  echo: false
---

# Introduction

[T]{.IEEEPARstart}[he]{} overall goal of this competition was to predict when two neurons will synapse or form a connection. As described in the competition guidelines, each observation in the data is a pair of neurons in axonaldendritic proximity, a necessary but not sufficient criterion to form a synapse. Each synapse occurs between a pre-synaptic neuron — whose axon is involved in the synapse — and a post-synaptic neuron (dendrite). Neurons can form synapses with multiple other neurons, and can form multiple synapses with a single other neuron. Neurons can be pre-synaptic in one ADP and post-synaptic in another. It would not make sense for a neuron to synapse with itself.

The distance between the axonal and dendritic ADP coordinates was provided, but other potential distances of interest were not. Given that our entire estimation procedure is a conditional one — when do neurons form a synapse _given_ they are in close proximity — I engineered features representing additional potential distances of interest.

Computational resources were likely to be strained, so I avoided introducing too many additional features. I also sought to compress the 32 morphological embedding and 512 feature weight measurements — which are joined with both of the pre-synaptic and post-synaptic neurons — to reduce the number of predictors and to avoid overfitting. I used three-fold cross-validation with creative tuning approaches such as Bayesian optimization and Latin hypercube sampling to explore as much of the hyperparameter space as realistically possible with constrained computation time. Then, I took the mode of the predictions of each of five models to finalize my submission.

# Methods

```{r}
#| message: false
raw_train <- readr::read_csv("neuron-synapse-prediction/train_data.csv") |>
	dplyr::mutate(connected = forcats::as_factor(connected) |>
									forcats::fct_relevel(c("TRUE", "FALSE")))
feature_weights <- readr::read_csv("neuron-synapse-prediction/feature_weights.csv")
morph_embeddings <- readr::read_csv("neuron-synapse-prediction/morph_embeddings.csv")
```

```{r}
missing_pre_morph <- raw_train |>
	dplyr::select(ID, pre_nucleus_id, post_nucleus_id) |>
	dplyr::left_join(morph_embeddings |>
									 	dplyr::select(nucleus_id, morph_pre = morph_emb_0),
									 by = c("pre_nucleus_id" = "nucleus_id")) |>
	dplyr::summarize(morph_pre = sum(is.na(morph_pre))/dplyr::n()) |>
	dplyr::pull()
```

## Feature engineering

### Imputation

My first feature engineering step was to impute the missing morphological embedding data. Though none of the post-synaptic neuron morphological embedding data were missing, we did not have morphological embeddings for `r scales::percent(missing_pre_morph)` of the pre-synaptic neurons. Instead of dropping rows, I imputed since some of the methods I used require no missing values.

There are many imputation methods, including but not limited to tree models, $K$ nearest-neighbors, and mean assignment. I chose linear regression imputation. Crucially, I did not have any neuroscientific understanding of these morphological embeddings, so I was operating somewhat in the dark. I noticed that each morphological embedding on its own looks to have a bell-shaped distribution with a mean around $0$ and a range from $-2$ to $2$, and that I could not discern many other patterns. I could have chosen a variety of imputation methods, each with their own hyperparameters, and tuned and validated them, but the marginal benefit to this intensive approach seemed minor. Linear regression is fast and simple, and especially because I was using summaries of each morphological embedding, I was satisfied.

### Distance metrics

Distance is clearly important in predicting when two neurons will form a synapse. Since we have multiple coordinates for each neuron, I decided to compute some of the relevant distances. The distance between the axonal and dendritic ADP coordinates was already given as `adp_dist`.

I computed the Euclidean distances between the axonal ADP and the post-synaptic nucleus, the dendritic ADP and the pre-synaptic nucleus, the pre-synaptic nucleus and post-synaptic nucleus, and the pre-synaptic readout location and post-synaptic readout location.

### Similarities and norms

I wanted to use as much information from the feature weights and morphological embeddings as possible without including every single predictor, which would amount to `r scales::comma((ncol(feature_weights) + ncol(morph_embeddings) - 2) * 2)` additional features. I settled on eight columns, four for feature weights and four for morphological embeddings. I computed the cosine similarity between the measures on the pre-synaptic and post-synaptic neurons. I also computed the norm of each of the pre-synaptic measures, the post-synaptic measures, and the element-wise difference between the pre-synaptic and post-synaptic measures.

Again, I brought no neuroscientific knowledge to this approach. Based on the competition documentation, I wondered if the magnitude of the feature weights and morphological embeddings mattered (since it said the sign was arbitrary), and I figured that neurons with similar measures may be connected, hence the norm of the element-wise difference.

### Popularity metrics


### Other feature engineering


## Logistic regression


## Random forest


## Gradient-boosted trees


## Stacking


# Results

Some results.


# Discussion

Discuss.


# Acknowledgements {-}

I worked alone and completed every step myself.


# References {-}

