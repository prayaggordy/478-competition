---
title: "Competition"
author: "Prayag Gordy"
format: html
---

To run the code, download `neuron-synapse-prediction` folder from Kaggle and put it at the project level.

```{r}
#| message: false
raw_train <- readr::read_csv("neuron-synapse-prediction/train_data.csv") |>
	dplyr::mutate(connected = forcats::as_factor(connected) |>
									forcats::fct_relevel(c("TRUE", "FALSE")))
feature_weights <- readr::read_csv("neuron-synapse-prediction/feature_weights.csv")
morph_embeddings <- readr::read_csv("neuron-synapse-prediction/morph_embeddings.csv")
```

```{r}
#| message: false
library(tidymodels)
library(doMC)
source("feature_engineering.R")
```

```{r}
set.seed(123)
trees_split <- initial_split(raw_train, strata = connected)
trees_train <- training(trees_split)
trees_test <- testing(trees_split)
```

```{r}
trees_train_fe <- do_feature_engineering(trees_train, fe_type = "training")
```

```{r}
trees_test_fe <- do_feature_engineering(trees_test, fe_type = "testing")
```

```{r}
tree_rec <- recipe(connected ~ ., data = trees_train_fe) |>
	update_role(ID, pre_nucleus_id, post_nucleus_id, new_role = "ID") |>
	themis::step_upsample(connected, over_ratio = 1)
```

## Logistic

```{r}
simple_logistic <- glm(connected ~ ., data = tree_rec |> prep() |> juice(), family = "binomial")
```

```{r}
test_pred <- predict(simple_logistic, trees_test_fe, type = "response") |>
	magrittr::is_less_than(0.5) |>
	factor()
```

```{r}
bal_accuracy(data.frame(x = factor(trees_test_fe$connected == "TRUE"),
												y = test_pred),
						 truth = x,
						 estimate = y)
```

```{r}
DF_TEST <- readr::read_csv("neuron-synapse-prediction/leaderboard_data.csv") |>
	do_feature_engineering(fe_type = "testing")
```

```{r}
TEST_PREDICTIONS <- predict(simple_logistic, DF_TEST, type = "response") |>
	magrittr::is_less_than(0.5) |>
	as.character() |>
	stringr::str_to_title()
```

```{r}
tibble::tibble(ID = DF_TEST$ID,
							 connected = TEST_PREDICTIONS) |>
	readr::write_csv("simple_logistic_preds.csv")
```

## Back to RF

```{r}
spec <- rand_forest(mode = "classification", engine = "ranger",
										mtry = tune(), trees = tune(), min_n = tune())
metric <- metric_set(bal_accuracy)
```

```{r}
set.seed(123)
rs <- vfold_cv(trees_train_fe, v = 3, strata = connected)
```

```{r}
wf <- workflow() |>
	add_recipe(tree_rec) |>
	add_model(spec)
```

```{r}
registerDoMC(cores = 7)

res_grid <- wf |> 
	tune_grid(resamples = rs, 
						metrics = metric,
						grid = tibble::tribble(
							~ "mtry", ~ "trees", ~ "min_n",
							3, 4000, 15000,
							4, 3500, 12000,
							4, 2500, 18000,
							5, 3000, 16000,
							5, 2200, 20000,
							6, 3200, 22000
						),
						control = control_grid(verbose = T))

registerDoSEQ()

save(res_grid, file = paste0("tune_grid_rf_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

```{r}
final_wf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(finalize_model(spec, select_best(res_grid)))

final_res <- final_wf |>
  fit(data = trees_train_fe)

save(final_res, file = paste0("tune_grid_rf_final_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

```{r}
final_res |>
  extract_fit_engine() |>
	predict(DF_TEST) |>
	purrr::pluck("predictions") |>
	tibble::as_tibble() |>
	dplyr::mutate(estimate = ifelse(`TRUE` > `FALSE`, "True", "False")) |>
	dplyr::bind_cols(DF_TEST |> dplyr::select(ID)) |>
	dplyr::select(ID, connected = estimate) |>
	readr::write_csv(paste0("PRED_tune_grid_rf_final_", format(Sys.time(), "%H_%M_%S"), ".csv"))
```

```{r}
final_res |>
  extract_fit_engine() |>
	predict(trees_test_fe) |>
	purrr::pluck("predictions") |>
	tibble::as_tibble() |>
	dplyr::mutate(estimate = ifelse(`TRUE` > `FALSE`, "TRUE", "FALSE") |> 
									factor(levels = c("TRUE", "FALSE"))) |> 
	dplyr::select(estimate) |> 
	dplyr::bind_cols(trees_test_fe |> 
									 	dplyr::select(connected)) |>
	bal_accuracy(truth = connected, estimate = estimate)
```



```{r}
load("tune_grid_rf_15_15_53.Rds")
```

```{r}
registerDoMC(cores = 7)

res <- wf |> 
	tune_bayes(resamples = rs, 
						 metrics = metric,
						 initial = res_grid,
						 param_info = parameters(mtry(range = c(2, 10)),
						 												trees(range = c(2000, 6000)),
						 												min_n(range = c(15000, 40000))),
						 control = control_bayes(verbose = TRUE))

registerDoSEQ()

save(res, file = paste0("tune_bayes_rf_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

```{r}
final_bayes_wf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(finalize_model(spec, select_best(res)))

final_bayes_res <- final_bayes_wf |>
  fit(data = trees_train_fe)

save(final_bayes_res, file = paste0("tune_bayes_rf_final_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

```{r}
final_bayes_res |>
  extract_fit_engine() |>
	predict(DF_TEST) |>
	purrr::pluck("predictions") |>
	tibble::as_tibble() |>
	dplyr::mutate(estimate = ifelse(`TRUE` > `FALSE`, "True", "False")) |>
	dplyr::bind_cols(DF_TEST |> dplyr::select(ID)) |>
	dplyr::select(ID, connected = estimate) |>
	readr::write_csv(paste0("PRED_tune_bayes_rf_final_", format(Sys.time(), "%H_%M_%S"), ".csv"))
```

## Boosting

```{r}
bt_rec <- recipe(connected ~ ., data = trees_train_fe) |>
	update_role(ID, pre_nucleus_id, post_nucleus_id, new_role = "ID") |>
	step_dummy(all_nominal_predictors()) |>
	step_mutate(projection_region_change = as.numeric(projection_region_change)) |>
	themis::step_upsample(connected, over_ratio = 1)

bt_spec <- boost_tree(trees = tune(), tree_depth = tune(),
											learn_rate = tune(), mtry = tune(), 
											min_n = tune(), loss_reduction = tune(),
											sample_size = tune()) |>
	set_mode("classification") |>
	set_engine("xgboost")

bt_grid <- grid_latin_hypercube(
	trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), trees_train_fe),
  learn_rate(),
  size = 30
)

metric <- metric_set(bal_accuracy)
```

```{r}
set.seed(123)
rs <- vfold_cv(trees_train_fe, v = 3, strata = connected)
```

```{r}
bt_wf <- workflow() |>
	add_recipe(bt_rec) |>
	add_model(bt_spec)
```

```{r}
registerDoMC(cores = 7)

res_bt <- bt_wf |> 
	tune_grid(resamples = rs, 
						metrics = metric,
						grid = bt_grid,
						control = control_grid(verbose = T))

registerDoSEQ()

save(res_bt, file = paste0("tune_grid_bt_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

```{r}
final_bt_wf <- workflow() |>
  add_recipe(bt_rec) |>
  add_model(finalize_model(bt_spec, select_best(res_bt)))

final_bt_res <- final_bt_wf |>
  fit(data = trees_train_fe)

save(final_bt_res, file = paste0("tune_grid_bt_final_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

```{r}
augment(final_bt_res, new_data = DF_TEST) |>
	dplyr::select(ID, .pred_class) |>
	dplyr::mutate(connected = as.character(.pred_class) |> stringr::str_to_title()) |>
	dplyr::select(ID, connected) |>
	readr::write_csv(paste0("PRED_tune_grid_bt_final_", format(Sys.time(), "%H_%M_%S"), ".csv"))
```

```{r}
registerDoMC(cores = 7)

res_bayes_bt <- bt_wf |> 
	tune_bayes(resamples = rs, 
						 metrics = metric,
						 initial = res_bt,
						 param_info = parameters(mtry(range = c(3, 44)), trees(range = c(1000, 4000)), 
						 												min_n(range = c(20, 60)), sample_prop(range = c(0.1, 1)),
						 												tree_depth(), learn_rate(), loss_reduction()),
						 control = control_bayes(verbose = TRUE))

registerDoSEQ()

save(res_bayes_bt, file = paste0("tune_bayes_bt_", format(Sys.time(), "%H_%M_%S"), ".Rds"))
```

